#!/bin/bash
#key_jade -N zmetamon
#key_jade -l select=<NB_NODES>:ncpus=<MPIPROC>:mpiprocs=<MPIPROC>
#key_jade -l walltime=<WALLTIME>
#key_jade -l cluster=hpt
#key_jade -M <MAIL>
#key_jade -mb -me
#key_jade -v NB_NODES=<NB_NODES>

#key_vayu -N zmetamon
#key_vayu -l ncpus=<MPIPROC>
#key_vayu -l walltime=<WALLTIME>
#key_vayu -M <MAIL>
#key_vayu -mb -me
#key_vayu -Wgroup_list=v45
#key_vayu -P x77
#key_vayu -q normal
#key_vayu -v NB_NODES=<NB_NODES>

#key_jean-zay --nodes=<NB_NODES>
#key_jean-zay --ntasks=<NB_NPROC>
#key_jean-zay --ntasks-per-node=<MPIPROC>
#key_jean-zay --threads-per-core=1
#key_jean-zay -A <ACCOUNT>
#key_jean-zay -J zmetamon
#key_jean-zay -e zmetamon.e%j
#key_jean-zay -o zmetamon.o%j
#key_jean-zay --time=<WALLTIME>
#key_jean-zay --exclusive

#SBATCH --nodes=<NB_NODES>
#SBATCH --ntasks=<NB_NPROC>
#SBATCH --ntasks-per-node=<MPIPROC>
#SBATCH --threads-per-core=1
#SBATCH -J zmetamon
#SBATCH -e zmetamon.e%j
#SBATCH -o zmetamon.o%j
#SBATCH --constraint=<NODETYPE>
#SBATCH --time=<WALLTIME>
#SBATCH --exclusive

#MSUB -r zmetamon
#MSUB -n  <NB_NPROC>
#MSUB -N  <NB_NODES>
#MSUB -T <WALLTIME>
#MSUB -q <QUEUE>
#MSUB -o zmetamon.%I
#MSUB -e zmetamon.%I
#MSUB -A <ACCOUNT>

#$ -N zmetamon 
#$ -cwd
#$ -j y
#$ -S /bin/bash
#$ -pe one <NB_NPROC>
#$ -v NB_NODES=<NB_NODES>

### LoadLeveler on ULAM and VARGAS
## title of the run
# @ job_name = zmetamon
## Output listing location
# @ output = $(job_name).$(jobid)
# @ error  = $(output)
# @ job_type = <JOBTYPE>
### DO NEVER ERASE THE FOLLOWING LINE
#ifloadlev# @ total_tasks = <NB_NPROC>
# specifique Adapp
# @ requirements = (Feature == "prepost")
# @ wall_clock_limit = <WALLTIME>
# @ as_limit = 3.2gb
# @ queue

## If R_MONITOR is a TMPDIR created by the job manager :
## the scripts copied by RUN_metamon are lost in the haze of a forgotten no man's land
## so we copy another time. If they are already there, it could do no harm

RNDTMPDIR=<RNDTMPDIR>

if [ $RNDTMPDIR == 1 ] ; then

cp ./config_def         $TMPDIR
cp ./function_def       $TMPDIR

cd $TMPDIR

. ./config_def
. ./function_def

cp $PRODTOOLS/create_sections_list    $TMPDIR
cp $PRODTOOLS/drakkar_sections_table.txt  $TMPDIR
cp $PRODTOOLS/drakkar_trpsig_table.txt    $TMPDIR
cp $PRODTOOLS/monitor_prod            $TMPDIR

if [ $useMPI == 1 ] ; then cp $MPITOOLS/mpi_metamon $TMPDIR ; fi

else

cd <R_MONITOR>

. ./config_def
. ./function_def

fi


# set the list of years you want to monitor 'at once'  
yinit=<year1>              # initial year 
yend=<year2>

YEARS=$( seq $yinit $yend )


if [ $useMPI = 1 ] ; then
### Yeah baby it is parallel !!!

if [ $MACHINE = 'jade' ] ; then
   mpiexec_mpt -n <NB_NPROC> ./mpi_metamon $YEARS
elif [ $MACHINE = 'occigen' ] ; then
   ulimit -s unlimited
   srun --mpi=pmi2 -n <NB_NPROC>  ./mpi_metamon $YEARS
elif [ $MACHINE = 'occigen2' -o $MACHINE = 'jean-zay' ] ; then
   ulimit -s unlimited
   srun --mpi=pmi2 -n <NB_NPROC>  ./mpi_metamon $YEARS
elif [ $MACHINE = 'curie' ] ; then
#  module unload netcdf
#   module unload hdf5
#   module load nco
   ccc_mprun -E '-m cyclic ' -n <NB_NPROC>  ./mpi_metamon $YEARS
elif [ $MACHINE = 'vayu' ] ; then
   mpirun -n <NB_NPROC>  ./mpi_metamon $YEARS
elif [ $MACHINE = 'gaia' ] ; then
   source $HOME/.bashrc
   mpirun -mca btl_tcp_if_include eth0 -np <NB_NPROC>  ./mpi_metamon $YEARS
elif [ $MACHINE = 'ulam' ] ; then
   ./mpi_metamon $YEARS
elif [ $MACHINE = 'ada' ] ; then
   poe ./mpi_metamon $YEARS
fi

else
### damn it is only sequential...
### this allows to ensure compatibility for most of the tags
  if [ ${#yinit} -gt 4 ] ; then
     # interannual plot
     chmod +x monitor_prod
     ./monitor_prod $yinit
  elif [ ${#yinit} -gt 4 ] ; then
     # standard modern year
     chmod +x monitor_prod
     ./monitor_prod $yinit
  else
     # climato runs
     yinit=$( printf "%04d" $yinit ) 
     chmod +x monitor_prod
     ./monitor_prod $yinit
  fi


fi
