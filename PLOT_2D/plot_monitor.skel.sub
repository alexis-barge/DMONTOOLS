#!/bin/bash
### PBS is valid on JADE
#jade -N zplotmonitor
#jade -l select=<NB_NODES>:ncpus=<MPIPROC>:mpiprocs=<MPIPROC>
#jade -l walltime=<PLOT_WALLTIME>
#jade -l cluster=hpt
#jade -M <MAIL>
#jade -mb -me
#jade -v NB_NODES=<NB_NODES>

#vayu -N zplotmonitor
#vayu -l select=<NB_NODES>:ncpus=<MPIPROC>:mpiprocs=<MPIPROC>
#vayu -l walltime=<PLOT_WALLTIME>
#vayu -l cluster=hpt
#vayu -M <MAIL>
#vayu -mb -me
#vayu -v NB_NODES=<NB_NODES>

#key_jean-zay --nodes=<NB_NODES>
#key_jean-zay --ntasks=<NB_NPROC>
#key_jean-zay --ntasks-per-node=<MPIPROC>
#key_jean-zay --threads-per-core=1
#key_jean-zay -A <ACCOUNT>
#key_jean-zay -J zmetamon
#key_jean-zay -e zmetamon.e%j
#key_jean-zay -o zmetamon.o%j
#key_jean-zay --time=<PLOT_WALLTIME>
#key_jean-zay --exclusive

#SBATCH --nodes=<NB_NODES>
#SBATCH --ntasks=<NB_NPROC>
#SBATCH --ntasks-per-node=<MPIPROC>
#SBATCH --threads-per-core=1
#SBATCH -J zplotmonitor
#SBATCH -e zplotmonitor.e%j
#SBATCH -o zplotmonitor.o%j
#SBATCH --constraint=<NODETYPE>
#SBATCH --time=<PLOT_WALLTIME>
#SBATCH --exclusive

### LoadLeveler on ULAM and VARGAS
## title of the run
# @ job_name = zplotmonitor
## Output listing location
# @ output = $(job_name).$(jobid)
# @ error  = $(output)
# @ job_type = <JOBTYPE>
# specifique Adapp
# @ requirements = (Feature == "prepost")
#ifloadlev# @ total_tasks = <NB_NPROC>
# @ wall_clock_limit = <PLOT_WALLTIME>
# @ as_limit = 3.2gb
# @ queue

### SLURM is valid on CURIE
#MSUB -r zplotmonitor
#MSUB -n   <NB_NPROC>              # Number of tasks to run            
#MSUB -N   <NB_NODES>              # Number of nodes to use          
#MSUB -q <QUEUE>
#MSUB -o zplotmonitor.%I
#MSUB -e zplotmonitor.%I
#MSUB -T <PLOT_WALLTIME>
#MSUB -A <ACCOUNT>

set -x
RNDTMPDIR=<RNDTMPDIR>

if [ $RNDTMPDIR == 1 ] ; then

cp ./config_def         $TMPDIR
cp ./function_def       $TMPDIR

cd $TMPDIR

. ./config_def
. ./function_def

cp -rf $PLOTTOOLS/PALDIR              $TMPDIR
cp     $PLOTTOOLS/plot_monitor    $TMPDIR

if [ $useMPI == 1 ] ; then cp $MPITOOLS/mpi_plot_monitor $TMPDIR ; fi

else

TMPDIR=$WORKDIR/GKSTMPDIR
mkdir -p $TMPDIR

cd <P_MONITOR>

. ./config_def
. ./function_def

fi

# set the list of years you want to monitor 'at once'  
yinit=<year1>
yend=<year2>
single=$( echo $yinit | awk '{ print index($1,"-") }' )


if [ $single = 0 ] ; then
  YEARS=$( seq $yinit $yend )
else
  YEARS=$yinit
fi

  
if [ $useMPI = 1 ] ; then
# MPI execution

  case $MACHINE in 
  ( jade )
    # mpi_plot_monitor $YEARS 
    #  Then each proc is expecting a command such as plot_monitor  $year
    if [ $single = 0 ] ; then
       mpiexec_mpt -n <NB_NPROC> ./mpi_plot_monitor $YEARS
    else
      ./plot_monitor $YEARS
    fi   ;;

  ( occigen | occigen2 | jean-zay)
    # mpi_plot_monitor $YEARS 
    #  Then each proc is expecting a command such as plot_monitor  $year
    if [ $single = 0 ] ; then
       srun --mpi=pmi2  -n <NB_NPROC> ./mpi_plot_monitor $YEARS
    else
      ./plot_monitor $YEARS
    fi   ;;


  ( vayu )
    # mpi_plot_monitor $YEARS
    #  Then each proc is expecting a command such as plot_monitor  $year
    if [ $single = 0 ] ; then
    mpirun  -n <NB_NPROC> ./mpi_plot_monitor $YEARS
    fi   ;;

  ( ada )
    if [ $single = 0 ] ; then
       poe ./mpi_plot_monitor $YEARS 
    else
      ./plot_monitor $YEARS
    fi   ;;

  ( ulam )
    ./mpi_plot_monitor $YEARS  ;;

  ( curie )
    if [ $single = 0 ] ; then
       ccc_mprun -E '-m cyclic ' -n <NB_NPROC>  ./mpi_plot_monitor $YEARS 
    else
      ./plot_monitor $YEARS
    fi   ;;


  ( * )
    echo No support for machine $MACHINE ;;
   esac

else

### this allows to ensure compatibility for most of the tags
  if [ ${#yinit} -gt 4 ] ; then
     # interannual plot
     ./plot_monitor $yinit
  elif [ ${#yinit} -gt 4 ] ; then
     # standard modern year
     ./plot_monitor $yinit
  else
     # climato runs
     yinit=$( printf "%04d" $yinit ) 
     ./plot_monitor $yinit
  fi

fi
